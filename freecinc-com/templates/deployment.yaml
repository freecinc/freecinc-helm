apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "freecinc-com.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "freecinc-com.name" . }}
    helm.sh/chart: {{ include "freecinc-com.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "freecinc-com.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  # Because the pods share a volume and that volume can only be mounted
  # on a single node, it is not possible to do a rolling update if the
  # new revision is scheduled on a different node.  So we recreate..
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "freecinc-com.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      volumes:
        - name: taskddata
          persistentVolumeClaim:
            claimName: {{ .Release.Name }}-taskddata
        - name: pki
          persistentVolumeClaim:
            claimName: {{ .Release.Name }}-pki
        - name: secrets
          secret:
            secretName: {{ .Release.Name }}-secrets
      containers:
        - name: freecinc-web
          image: "{{ .Values.webImage.repository }}:{{ .Values.webImage.tag }}"
          imagePullPolicy: {{ .Values.webImage.pullPolicy }}
          volumeMounts:
            - name: taskddata
              mountPath: "/taskddata"
            - name: pki
              mountPath: "/pki"
            - name: secrets
              mountPath: "/secrets"
          env:
            - name: TASKDDATA
              value: "/taskddata"
            - name: PKI_DIR
              value: "/pki"
            - name: SECRETS_DIR
              value: "/secrets"
            - name: APP_NAME
              value: {{ .Release.Name }}
            - name: SALT
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Name }}-secrets
                  key: salt
            - name: PUBLIC_HOSTNAME
              value: {{ required "hostname value must be given" .Values.hostname }}
{{- if eq .Release.Name "prod" }}
            - name: RACK_ENV
              value: production
{{- end }}
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            limits:
              cpu: 100m
              memory: 400Mi
            requests:
              cpu: 10m
              memory: 100Mi
        - name: freecinc-taskd
          image: "{{ .Values.taskdImage.repository }}:{{ .Values.taskdImage.tag }}"
          imagePullPolicy: {{ .Values.taskdImage.pullPolicy }}
          volumeMounts:
            - name: taskddata
              mountPath: "/taskddata"
            - name: secrets
              mountPath: "/secrets"
          env:
            - name: TASKDDATA
              value: "/taskddata"
            - name: SECRETS_DIR
              value: "/secrets"
          ports:
            - name: taskd
              containerPort: 53589
              protocol: TCP
          # taskd is single-threaded, so we wait a *long* time (15m) before marking this as unready or
          # not alive, and prefer liveness since there's only one pod at a time
          livenessProbe:
            tcpSocket:
              port: taskd
            initialDelaySeconds: 60
            periodSeconds: 300
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 1000Mi
            requests:
              cpu: 50m
              memory: 1000Mi
